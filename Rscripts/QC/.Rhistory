# import FastQC zip files
fdl = FastqcDataList(fdl)
fdl = list.files("Bureau/myprobe1/fastqc_before/",pattern = "fastqc.zip",full.names = T)
# import FastQC zip files
base_table <- getModule(fdl, "Basic_Statistics")
deduplicated_percentage <- getModule(fdl, "Total_Deduplicated_Percentage")
deduplicated_percentage["Total_Deduplicated_Percentage"] <- round(deduplicated_percentage["Total_Deduplicated_Percentage"], n_decimals)
n_decimals <- 2 # number of decimals
deduplicated_percentage <- getModule(fdl, "Total_Deduplicated_Percentage")
deduplicated_percentage["Total_Deduplicated_Percentage"] <- round(deduplicated_percentage["Total_Deduplicated_Percentage"], n_decimals)
base_table <- merge(base_table, deduplicated_percentage, by = "Filename")
base_table["Duplicated_Reads"] <- as.integer(unlist(((100 - base_table["Total_Deduplicated_Percentage"]) / 100) * base_table["Total_Sequences"]))
base_table["Unique_Reads"] <- base_table["Total_Sequences"] - base_table["Duplicated_Reads"]
base_table["%Duplicated_Reads"] <- round(base_table["Duplicated_Reads"] / base_table["Total_Sequences"] * 100, n_decimals)
base_table["%Unique_Reads"] <- round(base_table["Unique_Reads"] / base_table["Total_Sequences"] * 100, n_decimals)
actual_gc <- getModule(fdl,"Per_sequence_GC_content") #actual_gc contains gc content for all samples
for (sample_gc in levels(actual_gc$Filename)) {
sample_distribution = actual_gc$Count[actual_gc$Filename == sample_gc]
sample_distribution = (sample_distribution/sum(sample_distribution)) * 100
deviation_from_theorical = sum(abs(sample_distribution - theorical_gc))
}
for (sample_gc in levels(actual_gc$Filename)) {
sample_distribution = actual_gc$Count[actual_gc$Filename == sample_gc]
sample_distribution = (sample_distribution/sum(sample_distribution)) * 100
deviation_from_theorical = sum(abs(sample_distribution - theorical_gc))
print(c(sample_gc,deviation_from_theorical))
}
actual_gc <- getModule(fdl,"Per_sequence_GC_content") #actual_gc contains gc content for all samples
for (sample_gc in levels(actual_gc$Filename)) {
sample_distribution = actual_gc$Count[actual_gc$Filename == sample_gc]
sample_distribution = (sample_distribution/sum(sample_distribution)) * 100
deviation_from_theorical = sum(abs(sample_distribution - theorical_gc))
print(c(sample_gc,deviation_from_theorical))
}
for (sample_gc in levels(actual_gc$Filename)) {
sample_distribution = actual_gc$Count[actual_gc$Filename == sample_gc]
sample_distribution = (sample_distribution/sum(sample_distribution)) * 100
deviation_from_theorical = sum(abs(sample_distribution - theorical_gc))
message(c(sample_gc,deviation_from_theorical))
}
for (sample_gc in levels(actual_gc$Filename)) {
sample_distribution = actual_gc$Count[actual_gc$Filename == sample_gc]
sample_distribution = (sample_distribution/sum(sample_distribution)) * 100
deviation_from_theorical = sum(abs(sample_distribution - theorical_gc))
print(deviation_from_theorical)
}
View(actual_gc)
actual_gc["Filename"] = as.factor(actual_gc["Filename"])
for (sample_gc in levels(actual_gc$Filename)) {
sample_distribution = actual_gc$Count[actual_gc$Filename == sample_gc]
sample_distribution = (sample_distribution/sum(sample_distribution)) * 100
deviation_from_theorical = sum(abs(sample_distribution - theorical_gc))
print(deviation_from_theorical)
}
View(actual_gc)
actual_gc <- getModule(fdl,"Per_sequence_GC_content") #actual_gc contains gc content for all samples
View(actual_gc)
as.factor(actual_gc$Filename)
actual_gc["Filename"] = as.factor(actual_gc$Filename)
for (sample_gc in levels(actual_gc$Filename)) {
sample_distribution = actual_gc$Count[actual_gc$Filename == sample_gc]
sample_distribution = (sample_distribution/sum(sample_distribution)) * 100
deviation_from_theorical = sum(abs(sample_distribution - theorical_gc))
print(deviation_from_theorical)
}
theorical_distribution = read.csv("/home/theo/proj_hermes/RiboMethSeq-pipe/fastqc_theoretical_gc_hg38_genome.txt",sep = "\t")
theorical_gc = theorical_distribution
for (sample_gc in levels(actual_gc$Filename)) {
sample_distribution = actual_gc$Count[actual_gc$Filename == sample_gc]
sample_distribution = (sample_distribution/sum(sample_distribution)) * 100
deviation_from_theorical = sum(abs(sample_distribution - theorical_gc))
print(deviation_from_theorical)
}
theorical_distribution = read.csv("/home/theo/proj_hermes/RiboMethSeq-pipe/fastqc_theoretical_gc_hg38_txome.txt",sep = "\t")
theorical_gc = theorical_distribution
for (sample_gc in levels(actual_gc$Filename)) {
sample_distribution = actual_gc$Count[actual_gc$Filename == sample_gc]
sample_distribution = (sample_distribution/sum(sample_distribution)) * 100
deviation_from_theorical = sum(abs(sample_distribution - theorical_gc))
print(deviation_from_theorical)
}
actual_gc <- getModule(fdl,"Per_sequence_GC_content") #actual_gc contains gc content for all samples
actual_gc["Filename"] = as.factor(actual_gc$Filename)
for (sample_gc in levels(actual_gc$Filename)) {
sample_distribution = actual_gc$Count[actual_gc$Filename == sample_gc]
sample_distribution = (sample_distribution/sum(sample_distribution)) * 100
deviation_from_theorical = sum(abs(sample_distribution - theorical_gc))
print(deviation_from_theorical)
}
for (sample_gc in levels(actual_gc$Filename)) {
sample_distribution = actual_gc$Count[actual_gc$Filename == sample_gc]
sample_distribution = (sample_distribution/sum(sample_distribution)) * 100
deviation_from_theorical = sum(abs(sample_distribution - theorical_gc))
print(deviation_from_theorical)
}
#Calculating deviation percent from theorical distribution for GC Content, because FastQC seems to not keep them.
theorical_gc <- gcTheoretical@Genome$Hsapiens
actual_gc <- getModule(fdl,"Per_sequence_GC_content") #actual_gc contains gc content for all samples
actual_gc["Filename"] = as.factor(actual_gc$Filename)
for (sample_gc in levels(actual_gc$Filename)) {
sample_distribution = actual_gc$Count[actual_gc$Filename == sample_gc]
sample_distribution = (sample_distribution/sum(sample_distribution)) * 100
deviation_from_theorical = sum(abs(sample_distribution - theorical_gc))
print(deviation_from_theorical)
}
tbl["Status"] = ifelse(tbl["fail"],"FAIL")
fdl = list.files("Bureau/Novaseq_test2/", pattern = "*.zip", full.names = T)
# import FastQC zip files
base_table <- getModule(fdl, "Basic_Statistics")
deduplicated_percentage <- getModule(fdl, "Total_Deduplicated_Percentage")
deduplicated_percentage["Total_Deduplicated_Percentage"] <- round(deduplicated_percentage["Total_Deduplicated_Percentage"], n_decimals)
base_table <- merge(base_table, deduplicated_percentage, by = "Filename")
base_table["Duplicated_Reads"] <- as.integer(unlist(((100 - base_table["Total_Deduplicated_Percentage"]) / 100) * base_table["Total_Sequences"]))
base_table["Unique_Reads"] <- base_table["Total_Sequences"] - base_table["Duplicated_Reads"]
base_table["%Duplicated_Reads"] <- round(base_table["Duplicated_Reads"] / base_table["Total_Sequences"] * 100, n_decimals)
base_table["%Unique_Reads"] <- round(base_table["Unique_Reads"] / base_table["Total_Sequences"] * 100, n_decimals)
#Calculating deviation percent from theorical distribution for GC Content, because FastQC seems to not keep them.
theorical_gc <- gcTheoretical@Genome$Hsapiens
# Then, we import Trimmomatic logs with our modified parser in trimmomatic_parser.R
trim_logs_list <- list.files(path = input_datadir, pattern = "*.trimmomatic.stats.log", full.names = T, recursive = T)
data_trimmomatic <- suppressWarnings(lapply(trim_logs_list, readLines)) # load all lines for all trimmomatic logs
names(data_trimmomatic) <- basename(trim_logs_list)
# The parser can easily fail if the logs have not a correct structure
trim_logs <- tryCatch({
trim_logs <- parseTrimmomaticLogs(data_trimmomatic)
# Inside trim_logs, we have user-specified parameters used by trimmomatic. We will keep them in trim_parameters
trim_parameters <- t(trim_logs[1, -c(1:9)])
trim_parameters[is.na(trim_parameters)] <- "Not specified"
print("Trimmomatic logs have been successfully imported")
trim_logs <- trim_logs[, colSums(is.na(trim_logs)) != nrow(trim_logs)]
trim_logs["%Dropped"] <- trim_logs["Dropped"] / trim_logs["Input_Reads"] * 100
trim_logs["%Dropped"] <- round(trim_logs["%Dropped"], n_decimals)
trim_logs["%Surviving"] <- 100 - trim_logs["%Dropped"]
trim_logs["%Surviving"] <- round(trim_logs["%Surviving"], n_decimals)
trim_logs <- trim_logs[trimmomatic_selected_columns]
},
error = function(cond) {
message(paste("Trimmomatic parser failed to read logs !"))
message("Here's the original error message:")
message(cond)
# Choose a return value in case of error
return(NA)
},
warning = function(cond) {
message(paste("Trimmomatic parser issued a warning"))
message("Here's the original warning message:")
message(cond)
# Choose a return value in case of warning
return(NA)
},
finally = {
}
)
bowtie_selected_columns <- c("Filename", "Unique_Unpaired", "%Unique_Unpaired", "Multiple_Unpaired", "%Multiple_Unpaired", "Alignment_Rate", "Not_Aligned", "%Not_Aligned")
trimmomatic_selected_columns <- c("Filename", "Dropped", "%Dropped", "Surviving", "%Surviving")
# The parser can easily fail if the logs have not a correct structure
trim_logs <- tryCatch({
trim_logs <- parseTrimmomaticLogs(data_trimmomatic)
# Inside trim_logs, we have user-specified parameters used by trimmomatic. We will keep them in trim_parameters
trim_parameters <- t(trim_logs[1, -c(1:9)])
trim_parameters[is.na(trim_parameters)] <- "Not specified"
print("Trimmomatic logs have been successfully imported")
trim_logs <- trim_logs[, colSums(is.na(trim_logs)) != nrow(trim_logs)]
trim_logs["%Dropped"] <- trim_logs["Dropped"] / trim_logs["Input_Reads"] * 100
trim_logs["%Dropped"] <- round(trim_logs["%Dropped"], n_decimals)
trim_logs["%Surviving"] <- 100 - trim_logs["%Dropped"]
trim_logs["%Surviving"] <- round(trim_logs["%Surviving"], n_decimals)
trim_logs <- trim_logs[trimmomatic_selected_columns]
},
error = function(cond) {
message(paste("Trimmomatic parser failed to read logs !"))
message("Here's the original error message:")
message(cond)
# Choose a return value in case of error
return(NA)
},
warning = function(cond) {
message(paste("Trimmomatic parser issued a warning"))
message("Here's the original warning message:")
message(cond)
# Choose a return value in case of warning
return(NA)
},
finally = {
}
)
############ BOWTIE 2 ############
bowtie_logs_list <- list.files(path = input_datadir, pattern = "*.bowtie2.stats.log", full.names = T, recursive = T)
bowtie_logs <- importNgsLogs(bowtie_logs_list, type = "bowtie2")
print("bowtie logs have been successfully imported!")
bowtie_logs["Alignment_Rate"] <- round(bowtie_logs["Alignment_Rate"] * 100, n_decimals)
bowtie_logs["%Unique_Unpaired"] <- round(bowtie_logs["Unique_Unpaired"] / bowtie_logs["Total_Reads"] * 100, n_decimals)
bowtie_logs["%Multiple_Unpaired"] <- round(bowtie_logs["Multiple_Unpaired"] / bowtie_logs["Total_Reads"] * 100, n_decimals)
bowtie_logs["%Not_Aligned"] <- round(bowtie_logs["Not_Aligned"] / bowtie_logs["Total_Reads"] * 100, n_decimals) # P of reads not aligned
bowtie_logs <- bowtie_logs[bowtie_selected_columns]
print("Bowtie logs have been successfully imported")
# first, we rename the filename and then with merge the tables
base_table["Filename"] <- lapply(base_table["Filename"], sub, pattern = "_.*", replacement = "")
bowtie_logs["Filename"] <- lapply(bowtie_logs["Filename"], sub, pattern = "[.].*", replacement = "")
if (!is.na(trim_logs)) {
trim_logs["Filename"] <- lapply(trim_logs["Filename"], sub, pattern = "[.].*", replacement = "")
total_table <- merge(base_table, trim_logs, by = "Filename")
}
total_table <- merge(total_table, bowtie_logs, by = "Filename")
total_table["%Used_reads_for_counting"] <- round((total_table["Total_Sequences"] - total_table["Dropped"] - total_table["Not_Aligned"]) / total_table["Total_Sequences"] * 100, n_decimals)
FlagByValue <- function(tbl,flag_name,sample_col,val_col,warning_condition,fail_condition) {
tbl_vals = as.numeric(unlist(tbl[val_col]))
tbl["warning"] = which(warning_condition(tbl_vals))
tbl["fail"] = which(fail_condition(tbl_vals))
tbl[flag_name] = "PASS"
tbl[flag_name][tbl["warning"]] = "WARNING"
tbl[flag_name][tbl["fail"]] = "FAIL"
tbl = tbl[,c("Filename",flag_name)]
Category = flag_name
tbl_output = tidyr::gather(tbl, key = Category, value ="Status", one_of(Category))
return(tbl_output)
}
View(total_table)
FlagByValue(total_table,"Per sequence test","Filename","Total_Sequences",function(X) X < 30000000, function(X) <15000000)
FlagByValue(total_table,"Per sequence test","Filename","Total_Sequences",function(X) X < 30000000, function(X) X < 15000000)
FlagByValue(total_table,"Per sequence test","Filename","Total_Sequences",function(X) X < 30000000, function(X) X < 15000000)
debugSource('~/proj_hermes/RiboMethSeq-pipe/r_flagging.R')
FlagByValue(total_table,"Per sequence test","Filename","Total_Sequences",function(X) X < 30000000, function(X) X < 15000000)
View(tbl)
FlagByValue(total_table,"Per sequence test","Filename","Total_Sequences",function(X) X < 30000000, function(X) X < 15000000)
View(tbl)
FlagByValue(total_table,"Per sequence test","Filename","Total_Sequences",function(X)X<30000000, function(X)X<15000000)
View(tbl)
FlagByValue(total_table,"Per sequence test","Filename","Total_Sequences",function(x)x<30000000, function(x)x<15000000)
View(tbl)
FlagByValue(total_table,"Per sequence test","Filename","Total_Sequences",function(x) x<30000000, function(x) x<15000000)
debugSource('~/proj_hermes/RiboMethSeq-pipe/r_flagging.R')
FlagByValue(total_table,"Per sequence test","Filename","Total_Sequences",function(x) x<30000000, function(x) x<15000000)
View(tbl)
function(x) x < 45
FlagByValue(total_table,"Per sequence test","Filename","Total_Sequences",function(x) x<30000000, function(x) x<15000000)
View(fail_condition)
View(warning_condition)
View(fail_condition)
View(warning_condition)
FlagByValue(total_table,"Per sequence test","Filename","Total_Sequences",function(x) x<30000000, function(x) x<15000000)
FlagByValue(total_table,"Per sequence test","Filename","Total_Sequences",function(x) x<30000000, function(x) x<15000000)
which(warning_condition(tbl_vals))
which(warning_condition(12))
debugSource('~/proj_hermes/RiboMethSeq-pipe/r_flagging.R')
FlagByValue(total_table,"Per sequence test","Filename","Total_Sequences",function(x) x<30000000, function(x) x<15000000)
View(tbl)
debugSource('~/proj_hermes/RiboMethSeq-pipe/r_flagging.R')
FlagByValue(total_table,"Per sequence test","Filename","Total_Sequences",function(x) x<30000000, function(x) x<15000000)
View(tbl)
FlagByValue(total_table,"Per sequence test","Filename","Unique_Unpaired",function(x) x<30000000, function(x) x<15000000)
debugSource('~/proj_hermes/RiboMethSeq-pipe/r_flagging.R')
FlagByValue(total_table,"Per sequence test","Filename","Unique_Unpaired",function(x) x<30000000, function(x) x<15000000)
View(tbl)
View(tbl)
View(tbl)
FlagByValue(total_table,"Per sequence test","Filename","Unique_Unpaired",function(x) x<30000000, function(x) x<15000000)
View(tbl)
debugSource('~/proj_hermes/RiboMethSeq-pipe/r_flagging.R')
FlagByValue(total_table,"Per sequence test","Filename","Unique_Unpaired",function(x) x<30000000, function(x) x<15000000)
View(tbl)
View(tbl_output)
FlagByValue(total_table,"Per sequence test","Filename","Unique_Unpaired",function(x) x<30000000, function(x) x<15000000)
source('~/proj_hermes/RiboMethSeq-pipe/r_flagging.R')
source('~/proj_hermes/RiboMethSeq-pipe/r_flagging.R')
View(base_table)
View(total_table)
View(fdl_novaseq2)
adapt_module = getModule(fdl_novaseq2,"Adapter")
View(adapt_module)
adapt_module[which(adapt_module$Filename == "RMS1_1")]
adapt_module[adapt_module$Filename == "RMS1_1"]
adapt_module[adapt_module$Filename == "RMS1_1"]
adapt_module[which(adapt_module$Filename == "RMS1_1")]
adapt_module[which(adapt_module$Filename == "RMS1_1"),]
adapt_module[which(adapt_module["Filename"] == "RMS1_1"),]
RM1_adapt = adapt_module[which(adapt_module["Filename"] == "RMS1_1"),]
mean(RM1_adapt)
View(RM1_adapt)
mean(RM1_adapt[,3:4])
mean(RM1_adapt[,3:7])
mean(as.numeric(RM1_adapt[,3:7]))
mean(RM1_adapt$Illumina_Universal_Adapter)
head(RM1_adapt)
colnames(RM1_adapt)
sum(colMeans(RM1_adapt[,3:7]))
bowtie_selected_columns <- c("Filename", "Unique_Unpaired", "%Unique_Unpaired", "Multiple_Unpaired", "%Multiple_Unpaired", "Alignment_Rate", "Not_Aligned", "%Not_Aligned")
trimmomatic_selected_columns <- c("Filename", "Dropped", "%Dropped", "Surviving", "%Surviving")
fdl <- list.files(path = paste(input_datadir), pattern = "fastqc.zip", full.names = TRUE, recursive = T)
# import FastQC zip files
base_table <- getModule(fdl, "Basic_Statistics")
deduplicated_percentage <- getModule(fdl, "Total_Deduplicated_Percentage")
deduplicated_percentage["Total_Deduplicated_Percentage"] <- round(deduplicated_percentage["Total_Deduplicated_Percentage"], n_decimals)
base_table <- merge(base_table, deduplicated_percentage, by = "Filename")
base_table["Duplicated_Reads"] <- as.integer(unlist(((100 - base_table["Total_Deduplicated_Percentage"]) / 100) * base_table["Total_Sequences"]))
base_table["Unique_Reads"] <- base_table["Total_Sequences"] - base_table["Duplicated_Reads"]
base_table["%Duplicated_Reads"] <- round(base_table["Duplicated_Reads"] / base_table["Total_Sequences"] * 100, n_decimals)
base_table["%Unique_Reads"] <- round(base_table["Unique_Reads"] / base_table["Total_Sequences"] * 100, n_decimals)
#Adapter content
base_table["%Adapter_content"] = NA
adapter_content = getModule(fdl_novaseq2,"Adapter_Content")
adapter_content["Filename"] = as.factor(adapter_content$Filename)
for(sample_adapter in levels(adapter_content$Filename)) {
adapter_by_sample = adapter_content[which(adapter_content["Filename"] == sample_adapter),]
base_table["%Adapter_content"] = sum(colMeans(adapter_by_sample[,3:7]))
}
# Then, we import Trimmomatic logs with our modified parser in trimmomatic_parser.R
trim_logs_list <- list.files(path = input_datadir, pattern = "*.trimmomatic.stats.log", full.names = T, recursive = T)
View(base_table)
# import FastQC zip files
base_table <- getModule(fdl, "Basic_Statistics")
deduplicated_percentage <- getModule(fdl, "Total_Deduplicated_Percentage")
deduplicated_percentage["Total_Deduplicated_Percentage"] <- round(deduplicated_percentage["Total_Deduplicated_Percentage"], n_decimals)
base_table <- merge(base_table, deduplicated_percentage, by = "Filename")
base_table["Duplicated_Reads"] <- as.integer(unlist(((100 - base_table["Total_Deduplicated_Percentage"]) / 100) * base_table["Total_Sequences"]))
base_table["Unique_Reads"] <- base_table["Total_Sequences"] - base_table["Duplicated_Reads"]
base_table["%Duplicated_Reads"] <- round(base_table["Duplicated_Reads"] / base_table["Total_Sequences"] * 100, n_decimals)
base_table["%Unique_Reads"] <- round(base_table["Unique_Reads"] / base_table["Total_Sequences"] * 100, n_decimals)
#Adapter content
base_table["%Adapter_content"] = NA
adapter_content = getModule(fdl,"Adapter_Content")
adapter_content["Filename"] = as.factor(adapter_content$Filename)
for(sample_adapter in levels(adapter_content$Filename)) {
adapter_by_sample = adapter_content[which(adapter_content["Filename"] == sample_adapter),]
base_table["%Adapter_content"] = sum(colMeans(adapter_by_sample[,3:7]))
}
View(base_table)
for(sample_adapter in levels(adapter_content$Filename)) {
adapter_by_sample = adapter_content[which(adapter_content["Filename"] == sample_adapter),]
base_table["%Adapter_content"][base_table["Filename"] == sample_adapter] = sum(colMeans(adapter_by_sample[,3:7]))
}
View(base_table)
View(base_table)
if (!is.na(trim_logs)) {
trim_logs["Filename"] <- lapply(trim_logs["Filename"], sub, pattern = "[.].*", replacement = "")
total_table <- merge(base_table, trim_logs, by = "Filename")
}
data_trimmomatic <- suppressWarnings(lapply(trim_logs_list, readLines)) # load all lines for all trimmomatic logs
names(data_trimmomatic) <- basename(trim_logs_list)
# The parser can easily fail if the logs have not a correct structure
trim_logs <- tryCatch({
trim_logs <- parseTrimmomaticLogs(data_trimmomatic)
# Inside trim_logs, we have user-specified parameters used by trimmomatic. We will keep them in trim_parameters
trim_parameters <- t(trim_logs[1, -c(1:9)])
trim_parameters[is.na(trim_parameters)] <- "Not specified"
print("Trimmomatic logs have been successfully imported")
trim_logs <- trim_logs[, colSums(is.na(trim_logs)) != nrow(trim_logs)]
trim_logs["%Dropped"] <- trim_logs["Dropped"] / trim_logs["Input_Reads"] * 100
trim_logs["%Dropped"] <- round(trim_logs["%Dropped"], n_decimals)
trim_logs["%Surviving"] <- 100 - trim_logs["%Dropped"]
trim_logs["%Surviving"] <- round(trim_logs["%Surviving"], n_decimals)
trim_logs <- trim_logs[trimmomatic_selected_columns]
},
error = function(cond) {
message(paste("Trimmomatic parser failed to read logs !"))
message("Here's the original error message:")
message(cond)
# Choose a return value in case of error
return(NA)
},
warning = function(cond) {
message(paste("Trimmomatic parser issued a warning"))
message("Here's the original warning message:")
message(cond)
# Choose a return value in case of warning
return(NA)
},
finally = {
}
)
############ BOWTIE 2 ############
bowtie_logs_list <- list.files(path = input_datadir, pattern = "*.bowtie2.stats.log", full.names = T, recursive = T)
bowtie_logs <- importNgsLogs(bowtie_logs_list, type = "bowtie2")
print("bowtie logs have been successfully imported!")
bowtie_logs["Alignment_Rate"] <- round(bowtie_logs["Alignment_Rate"] * 100, n_decimals)
bowtie_logs["%Unique_Unpaired"] <- round(bowtie_logs["Unique_Unpaired"] / bowtie_logs["Total_Reads"] * 100, n_decimals)
bowtie_logs["%Multiple_Unpaired"] <- round(bowtie_logs["Multiple_Unpaired"] / bowtie_logs["Total_Reads"] * 100, n_decimals)
bowtie_logs["%Not_Aligned"] <- round(bowtie_logs["Not_Aligned"] / bowtie_logs["Total_Reads"] * 100, n_decimals) # P of reads not aligned
bowtie_logs <- bowtie_logs[bowtie_selected_columns]
print("Bowtie logs have been successfully imported")
# first, we rename the filename and then with merge the tables
base_table["Filename"] <- lapply(base_table["Filename"], sub, pattern = "_.*", replacement = "")
bowtie_logs["Filename"] <- lapply(bowtie_logs["Filename"], sub, pattern = "[.].*", replacement = "")
if (!is.na(trim_logs)) {
trim_logs["Filename"] <- lapply(trim_logs["Filename"], sub, pattern = "[.].*", replacement = "")
total_table <- merge(base_table, trim_logs, by = "Filename")
}
total_table <- merge(total_table, bowtie_logs, by = "Filename")
total_table["%Used_reads_for_counting"] <- round((total_table["Total_Sequences"] - total_table["Dropped"] - total_table["Not_Aligned"]) / total_table["Total_Sequences"] * 100, n_decimals)
View(total_table)
View(adapt_content)
View(RM1_adapt)
View(fdl_novaseq2)
View(fdl_novaseq2[[1]]@Per_base_N_content)
View(fdl_novaseq2[[1]]@Per_sequence_quality_scores)
View(fdl_novaseq2[[1]]@Overrepresented_sequences)
View(fdl_novaseq2)
View(fdl_novaseq2[[1]]@Adapter_Content)
median(RM1_adapt$Illumina_Universal_Adapter)
mean(RM1_adapt$Illumina_Universal_Adapter)
tail(RM1_adapt$`Illumina_Small_RNA_3'_Adapter`)
tail(RM1_adapt$Illumina_Universal_Adapter)
tail(RM1_adapt$Illumina_Universal_Adapter,1)
tail(RM1_adapt,1)
View(fdl_novaseq2)
flags_table = data.frame(c("Filename","Category","Status"))
View(flags_table)
flags_table = data.frame(,c("Filename","Category","Status"))
flags_summary <- data.frame()
colnames(flags_summary) <- c("Filename","Category","Status")
flags_summary <- data.frame(ncol = 3)
colnames(flags_summary) <- c("Filename","Category","Status")
View(flags_table)
View(flags_summary)
flags_summary <- data.frame(matrix(ncol = 3,nrow = 0))
colnames(flags_summary) <- c("Filename","Category","Status")
View(flags_summary)
library(dplyr)
View(fdl_novaseq2)
sequence_quality = getModule(fdl_novaseq2,)
sequence_quality = getModule(fdl_novaseq2)
sequence_quality = getModule(fdl_novaseq2,"e")
sequence_quality = getModule(fdl_novaseq2,"Per_sequence_quality_scores")
View(sequence_quality)
View(sequence_quality)
View(sequence_quality)
sequence_quality_scores = sequence_quality
for (sample_sq in levels(sequence_quality_scores$Filename)) {
sq_bySample = sequence_quality_scores[which(sequence_quality_scores["Filename"] == sample_adapter),]
quality_maxCount = sq_bySample$Quality[which(max(sq_bySample$count))]
print(quality_maxCount)
base_table["Per_sequence_quality_scores"][base_table["Filename"] == sample_sq] = quality_maxCount
}
View(base_table)
for (sample_sq in levels(sequence_quality_scores$Filename)) {
sq_bySample = sequence_quality_scores[which(sequence_quality_scores["Filename"] == sample_adapter),]
quality_maxCount = sq_bySample$Quality[which(max(sq_bySample$count))]
message(quality_maxCount)
base_table["Per_sequence_quality_scores"][base_table["Filename"] == sample_sq] = quality_maxCount
}
levels(sequence_quality_scores$Filename)
View(sequence_quality)
levels(as.factor(sequence_quality_scores$Filename)
)
for (sample_sq in levels(as.factor(sequence_quality_scores$Filename))) {
sq_bySample = sequence_quality_scores[which(sequence_quality_scores["Filename"] == sample_adapter),]
quality_maxCount = sq_bySample$Quality[which(max(sq_bySample$count))]
quality_maxCount
base_table["Per_sequence_quality_scores"][base_table["Filename"] == sample_sq] = quality_maxCount
}
View(base_table)
for (sample_sq in levels(as.factor(sequence_quality_scores$Filename))) {
sq_bySample = sequence_quality_scores[which(sequence_quality_scores["Filename"] == sample_adapter),]
quality_maxCount = sq_bySample$Quality[max(sq_bySample$count)]
quality_maxCount
base_table["Per_sequence_quality_scores"][base_table["Filename"] == sample_sq] = quality_maxCount
}
for (sample_sq in levels(as.factor(sequence_quality_scores$Filename))) {
sq_bySample = sequence_quality_scores[which(sequence_quality_scores["Filename"] == sample_adapter),]
quality_maxCount = sq_bySample$Quality[max(sq_bySample$Count)]
quality_maxCount
base_table["Per_sequence_quality_scores"][base_table["Filename"] == sample_sq] = quality_maxCount
}
max(sq_bySample$Count)
quality_maxCount = sq_bySample$Quality[max(sq_bySample["Count"])]
for (sample_sq in levels(as.factor(sequence_quality_scores$Filename))) {
sq_bySample = sequence_quality_scores[which(sequence_quality_scores["Filename"] == sample_adapter),]
quality_maxCount = sq_bySample$Quality[max(sq_bySample["Count"])]
quality_maxCount
base_table["Per_sequence_quality_scores"][base_table["Filename"] == sample_sq] = quality_maxCount
}
for (sample_sq in levels(as.factor(sequence_quality_scores$Filename))) {
sq_bySample = sequence_quality_scores[which(sequence_quality_scores["Filename"] == sample_sq),]
quality_maxCount = sq_bySample$Quality[max(sq_bySample["Count"])]
quality_maxCount
base_table["Per_sequence_quality_scores"][base_table["Filename"] == sample_sq] = quality_maxCount
}
View(sq_bySample)
sq_bySample$Quality[max(sq_bySample$Count)]
sq_bySample$Quality[which(max(sq_bySample$Count))]
sq_bySample$Quality[which(sq_bySample == max(sq_bySample$Count))]
sq_bySample$Quality[which(sq_bySample$Count == max(sq_bySample$Count))]
GetOutliers <- function(tbl,sample_col,val_col, up_low, threshold) {
#the list cannot be coerced to type "double". So we unlist first
tbl_val_unlisted = as.numeric(unlist(tbl[val_col]))
tbl_median = median(tbl_val_unlisted)
tbl_mad <- threshold*mad(tbl_val_unlisted)
tbl_UB = tbl_median + tbl_mad
tbl_LB = tbl_median - tbl_mad
#We flag with "FAIL" outliers...
#...above upper bound
tbl_outliers_UB_name = paste(val_col,"_outliers_UB",sep = "")
tbl[tbl_outliers_UB_name] = tbl[val_col] > tbl_UB
#...Below lower bound
tbl_outliers_LB_name = paste(val_col,"_outliers_LB",sep = "")
tbl[tbl_outliers_LB_name] = tbl[val_col] < tbl_LB
#according to up_low parameter, three different dataframes can be returned :
# 1) sample + outliers below lower bound ("Lower")
# 2) sample + outliers above upper bound ("Upper")
# 3) sample + outliers above upper bound + outliers below lower bound
col_to_keep = switch (up_low,
"Upper" = tbl_outliers_UB_name,
"Lower" = tbl_outliers_LB_name,
"Both"  = c(tbl_outliers_LB_name,tbl_outliers_UB_name)
)
tbl["is_outlier"] = tbl[tbl_outliers_UB_name] || tbl[tbl_outliers_LB_name]
tbl = tbl[c("Filename",col_to_keep,"is_outlier")]
return(tbl)
}
as.numeric(TRUE)
as.numeric(FALSE)
setwd("~/proj_hermes/RiboMethSeq-pipe/Rscripts/QC")
